{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os \n",
    "import sys\n",
    "sys.path.append(\"/mnt/RAID/projects/FjordVision\")\n",
    "from models.probability_tree import ProbabilityTree\n",
    "import torch\n",
    "from anytree.importer import JsonImporter\n",
    "from preprocessing.preprocessing import load_ground_truth_mask_xyn, convert_polygon_to_mask, calculate_binary_mask_iou\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import islice\n",
    "\n",
    "# Function to divide the data into chunks of size n\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "# Define the file path\n",
    "weights_path = '/mnt/RAID/projects/FjordVision/runs/segment/Yolov8n-seg-train/weights/best.pt'\n",
    "\n",
    "# Load the YOLO model weights\n",
    "model = YOLO(weights_path)\n",
    "\n",
    "importer = JsonImporter()\n",
    "with open('data/ontology.json', 'r') as f:\n",
    "    root = importer.read(f)\n",
    "\n",
    "classes_file = '/mnt/RAID/datasets/label-studio/fjord/classes.txt'\n",
    "\n",
    "species_names = []\n",
    "with open(classes_file, 'r') as file:\n",
    "    species_names = [line.strip() for line in file]\n",
    "\n",
    "genus_names, class_names, binary_names = [], [], []\n",
    "for node in root.descendants:\n",
    "    if node.rank == 'genus':\n",
    "        genus_names.append(node.name)\n",
    "    elif node.rank == 'class':\n",
    "        class_names.append(node.name)\n",
    "    elif node.rank == 'binary':\n",
    "        binary_names.append(node.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_labels(image_path, label_path, class_index):\n",
    "    # Load the image\n",
    "    image = plt.imread(image_path)\n",
    "    \n",
    "    # Create a figure and axes\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Plot the image\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    # Read the label file\n",
    "    with open(label_path, 'r') as file:\n",
    "        # Loop through the lines in the label file\n",
    "        for line in file:\n",
    "            # Split the line into class_id and coordinates\n",
    "            class_id, *coordinates = line.strip().split(' ')\n",
    "            \n",
    "            # Convert the coordinates to float and normalize them\n",
    "            coordinates = [float(coord) for coord in coordinates]\n",
    "            coordinates = [coord * image.shape[1] if i % 2 == 0 else coord * image.shape[0] for i, coord in enumerate(coordinates)]\n",
    "            \n",
    "            # Reshape the coordinates into an array of points for the polygon\n",
    "            points = np.array(coordinates).reshape(-1, 2)\n",
    "            \n",
    "            # Get the class label from the class_index\n",
    "            class_label = class_index[int(class_id)]\n",
    "            \n",
    "            # Get a unique color for each class\n",
    "            color = plt.cm.tab10(int(class_id) % 10)\n",
    "            \n",
    "            # Plot the polygon with the class label and color\n",
    "            polygon = plt.Polygon(points, edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(polygon)\n",
    "            ax.text(points[0, 0], points[0, 1], class_label, color=color, fontsize=8, verticalalignment='top')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_masks(image, masks, classes):\n",
    "    # Create a figure and axes\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Plot the image\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    # Loop through the masks and coordinates\n",
    "    for mask, cls in zip(masks, classes):    \n",
    "        mask[:, 0] = mask[:, 0] * image.shape[1]\n",
    "        mask[:, 1] = mask[:, 1] * image.shape[0]    \n",
    "        points = mask\n",
    "\n",
    "        # Get the class label from the class_index\n",
    "        class_label = species_names[int(cls)]\n",
    "        \n",
    "        # Get a unique color for each class\n",
    "        color = plt.cm.tab10(int(cls) % 10)\n",
    "        \n",
    "        # Plot the polygon with the class label and color\n",
    "        polygon = plt.Polygon(points, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(polygon)\n",
    "        ax.text(points[0, 0], points[0, 1], class_label, color=color, fontsize=8, verticalalignment='top')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Probability Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "ontology_path = 'data/ontology.json'  # Update this path as necessary\n",
    "prob_tree = ProbabilityTree(ontology_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the image folder path\n",
    "image_folder_path = '/mnt/RAID/datasets/The Fjord Dataset/fjord/images/test/'\n",
    "frames = os.listdir(image_folder_path)\n",
    "image_files_full_path = [image_folder_path + f for f in frames]\n",
    "\n",
    "# Define the label folder path\n",
    "label_folder_path = '/mnt/RAID/datasets/The Fjord Dataset/fjord/labels/test/'\n",
    "\n",
    "classes = '/mnt/RAID/datasets/The Fjord Dataset/fjord/classes.txt'\n",
    "\n",
    "class_index = []\n",
    "with open(classes, 'r') as file:\n",
    "    for line_number, line in enumerate(file, start=1):\n",
    "        class_name = line.strip()\n",
    "        class_index.append(class_name)\n",
    "\n",
    "\n",
    "Y = []\n",
    "Yhat = []\n",
    "batch_size = 50\n",
    "\n",
    "# Loop through batches of images\n",
    "for image_batch in chunks(image_files_full_path, batch_size):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_batch, stream=True)\n",
    "\n",
    "    # Loop through the files in the image folder\n",
    "    for file_name, prediction in zip(image_batch, predictions):\n",
    "        # Check if the file is an image file\n",
    "        if file_name.endswith('.jpg') or file_name.endswith('.png'):\n",
    "            # Construct the corresponding label file name\n",
    "            shape = prediction.orig_img.shape[:2]\n",
    "            base_file_name = file_name.split('/')[-1].replace('.jpg', '.txt')\n",
    "            label_file_path = label_folder_path + base_file_name\n",
    "        \n",
    "            # check if predictions are empty\n",
    "            if len(prediction.boxes.cls) == 0:\n",
    "                continue\n",
    "\n",
    "            GT = load_ground_truth_mask_xyn(label_file_path)\n",
    "            visited = len(GT)*[None]\n",
    "\n",
    "            for cls, mask in zip(prediction.boxes.cls, prediction.masks.xyn):\n",
    "                m = convert_polygon_to_mask(mask, shape)\n",
    "                best_iou = 0\n",
    "\n",
    "                # calculate iou and find the best mask\n",
    "                for idx, (gcls, gmsk) in enumerate(GT):\n",
    "                    g = convert_polygon_to_mask(gmsk, shape)\n",
    "                    iou = calculate_binary_mask_iou(m, g)\n",
    "\n",
    "                    if iou > best_iou and iou > 0.5:\n",
    "                        best_iou = iou\n",
    "                        best_g = g\n",
    "                        best_gcls = gcls\n",
    "                        visited[idx] = True\n",
    "                        best_idx = idx\n",
    "\n",
    "                if best_idx is not None and best_iou > 0.5:\n",
    "                    visited[best_idx] = True\n",
    "\n",
    "                if best_g is None:\n",
    "                    Y.append(None)\n",
    "                    Yhat.append(int(cls.item()))\n",
    "                else:\n",
    "                    Y.append(best_gcls)\n",
    "                    Yhat.append(int(cls.item()))\n",
    "\n",
    "            for vis in visited:\n",
    "                if vis is None:\n",
    "                    Y.append(GT[idx][0])\n",
    "                    Yhat.append(None)\n",
    "\n",
    "    # After processing each batch, clear unused memory from CUDA\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anytree.search import find\n",
    "from anytree.walker import Walker\n",
    "\n",
    "def hierarchical_similarity(node1, node2, tree):\n",
    "    walker = Walker()\n",
    "    # Assuming node1 and node2 are already the correct nodes from the tree\n",
    "    upwards, _, down = walker.walk(node1, node2)\n",
    "    distance = len(upwards) + len(down)\n",
    "    return 1 / (1 + distance)\n",
    "\n",
    "def calculate_hierarchical_precision_recall(Y, Yhat, tree, species_names):\n",
    "    weighted_true_positives = 0\n",
    "    weighted_false_positives = 0\n",
    "    weighted_false_negatives = 0\n",
    "\n",
    "    for true_label, predicted_label in zip(Y, Yhat):\n",
    "\n",
    "        if predicted_label is None:  # Handle negative prediction as complete miss\n",
    "            weighted_false_negatives += 1  # Might need to adjust based on how you want to treat negative predictions\n",
    "            continue\n",
    "        if true_label is None:  # Handle missing ground truth as complete miss\n",
    "            weighted_false_positives += 1\n",
    "            continue\n",
    "\n",
    "        node1 = find(tree, lambda node: node.name == species_names[true_label])\n",
    "        node2 = find(tree, lambda node: node.name == species_names[predicted_label])\n",
    "        similarity_weight = hierarchical_similarity(node1, node2, tree)\n",
    "\n",
    "        if true_label == predicted_label:\n",
    "            weighted_true_positives += similarity_weight\n",
    "        else:\n",
    "            weighted_false_positives += (1 - similarity_weight)  # This assumes you want to penalize based on dissimilarity\n",
    "\n",
    "    precision = weighted_true_positives / (weighted_true_positives + weighted_false_positives) if (weighted_true_positives + weighted_false_positives) > 0 else 0\n",
    "    recall = weighted_true_positives / (weighted_true_positives + weighted_false_negatives) if (weighted_true_positives + weighted_false_negatives) > 0 else 0\n",
    "    \n",
    "    return precision, recall\n",
    "\n",
    "def calculate_weighted_f1_score(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Calculate weighted precision, recall, and F1\n",
    "precision, recall = calculate_hierarchical_precision_recall(Y, Yhat, root, species_names)\n",
    "weighted_f1_score = calculate_weighted_f1_score(precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9588657803141181"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8972809667673716"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9270517169110761"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_f1_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fjordvision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
