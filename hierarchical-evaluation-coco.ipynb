{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os \n",
    "import sys\n",
    "sys.path.append(\"/mnt/RAID/projects/FjordVision\")\n",
    "from models.probability_tree import ProbabilityTree\n",
    "import torch\n",
    "from anytree.importer import JsonImporter\n",
    "from preprocessing.preprocessing import load_ground_truth_mask_xyn, convert_polygon_to_mask, calculate_binary_mask_iou\n",
    "from utils.metrics import calculate_hierarchical_precision_recall, calculate_weighted_f1_score, hierarchical_similarity\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Function to divide the data into chunks of size n\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "# Load the YOLO model weights\n",
    "model = YOLO('yolov8n-seg.pt')\n",
    "\n",
    "importer = JsonImporter()\n",
    "with open('data/coco.json', 'r') as f:\n",
    "    root = importer.read(f)\n",
    "\n",
    "classes_file = '/mnt/RAID/datasets/coco/classes.txt'\n",
    "\n",
    "cls_names = []\n",
    "with open(classes_file, 'r') as file:\n",
    "    cls_names = [line.strip() for line in file]\n",
    "\n",
    "subcat_names, cat_names, root_names = [], [], []\n",
    "for node in root.descendants:\n",
    "    if node.rank == 'subcategory':\n",
    "        subcat_names.append(node.name)\n",
    "    elif node.rank == 'category':\n",
    "        cat_names.append(node.name)\n",
    "    elif node.rank == 'root':\n",
    "        root_names.append(node.name)\n",
    "\n",
    "taxonomies = [cls_names, subcat_names, cat_names, root_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Probability Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "ontology_path = 'data/coco.json'  # Update this path as necessary\n",
    "prob_tree = ProbabilityTree(ontology_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the image folder path\n",
    "image_folder_path = '/mnt/RAID/datasets/COCO/images/val2017/'\n",
    "frames = os.listdir(image_folder_path)\n",
    "image_files_full_path = [image_folder_path + f for f in frames]\n",
    "\n",
    "# Define the label folder path\n",
    "label_folder_path = '/mnt/RAID/datasets/COCO/labels/val2017/'\n",
    "\n",
    "class_index = []\n",
    "with open(classes_file, 'r') as file:\n",
    "    for line_number, line in enumerate(file, start=1):\n",
    "        class_name = line.strip()\n",
    "        class_index.append(class_name)\n",
    "\n",
    "Y = []\n",
    "Yhat = []\n",
    "confidences = []\n",
    "batch_size = 50\n",
    "\n",
    "# Loop through batches of images\n",
    "for image_batch in chunks(image_files_full_path, batch_size):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_batch, stream=True)\n",
    "\n",
    "    # Loop through the files in the image folder\n",
    "    for file_name, prediction in zip(image_batch, predictions):\n",
    "        # Check if the file is an image file\n",
    "        if file_name.endswith('.jpg') or file_name.endswith('.png'):\n",
    "            # Construct the corresponding label file name\n",
    "            shape = prediction.orig_img.shape[:2]\n",
    "            base_file_name = file_name.split('/')[-1].replace('.jpg', '.txt')\n",
    "            label_file_path = label_folder_path + base_file_name\n",
    "            NOGT = False\n",
    "            # check if predictions are empty\n",
    "            if len(prediction.boxes.cls) == 0:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                GT = load_ground_truth_mask_xyn(label_file_path)\n",
    "            except FileNotFoundError:\n",
    "                NOGT = True\n",
    "                \n",
    "            visited = len(GT)*[None]\n",
    "\n",
    "            for cls, mask, conf in zip(prediction.boxes.cls, prediction.masks.xyn, prediction.boxes.conf):\n",
    "                confidences.append(conf.item())\n",
    "                m = convert_polygon_to_mask(mask, shape)\n",
    "                best_iou = 0\n",
    "\n",
    "                if NOGT:\n",
    "                    Y.append(None)\n",
    "                    Yhat.append(int(cls.item()))\n",
    "                    continue\n",
    "                else:\n",
    "                    # calculate iou and find the best mask\n",
    "                    for idx, (gcls, gmsk) in enumerate(GT):\n",
    "                        g = convert_polygon_to_mask(gmsk, shape)\n",
    "                        iou = calculate_binary_mask_iou(m, g)\n",
    "\n",
    "                        if iou > best_iou and iou > 0.5:\n",
    "                            best_iou = iou\n",
    "                            best_g = g\n",
    "                            best_gcls = gcls\n",
    "                            visited[idx] = True\n",
    "                            best_idx = idx\n",
    "\n",
    "                if best_idx is not None and best_iou > 0.5:\n",
    "                    visited[best_idx] = True\n",
    "\n",
    "                if best_g is None:\n",
    "                    Y.append(None)\n",
    "                    Yhat.append(int(cls.item()))\n",
    "                else:\n",
    "                    Y.append(best_gcls)\n",
    "                    Yhat.append(int(cls.item()))\n",
    "\n",
    "            for vis in visited:\n",
    "                if vis is None:\n",
    "                    Y.append(GT[idx][0])\n",
    "                    Yhat.append(None)\n",
    "\n",
    "    # After processing each batch, clear unused memory from CUDA\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate scores without reclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted precision, recall, and F1\n",
    "precision, recall = calculate_hierarchical_precision_recall(Y, Yhat, confidences, taxonomies, prob_tree, threshold=0)\n",
    "weighted_f1_score = calculate_weighted_f1_score(precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8116082735331355"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5414607358594179"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6495662970126433"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Predictions with uniform probability tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted precision, recall, and F1\n",
    "precision, recall = calculate_hierarchical_precision_recall(Y, Yhat, confidences, taxonomies, prob_tree, threshold=0.3)\n",
    "weighted_f1_score = calculate_weighted_f1_score(precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores with reclassifiation using uniform tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7882423271710082"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5176027232892776"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6248771628942967"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_f1_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fjordvision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
