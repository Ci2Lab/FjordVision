Experiment started at Tue Jun 25 09:52:04 CEST 2024
Running increased_features_complexity experiments:
Running increased_features_complexity classification training with alpha = 0 using script scripts/increased_features_complexity/hierarchical-classification-training.py
/home/bcw/gits/FjordVision/fjordvision/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
Epoch: 1, Training Loss: 4.6854, Validation Loss: 4.2059, Alpha: 0.0000
Epoch: 2, Training Loss: 3.6058, Validation Loss: 3.1175, Alpha: 0.0000
Epoch: 3, Training Loss: 2.8170, Validation Loss: 2.3904, Alpha: 0.0000
Epoch: 4, Training Loss: 2.2897, Validation Loss: 1.8069, Alpha: 0.0000
Epoch: 5, Training Loss: 1.9810, Validation Loss: 1.5747, Alpha: 0.0000
Epoch: 6, Training Loss: 1.7371, Validation Loss: 1.2964, Alpha: 0.0000
Epoch: 7, Training Loss: 1.5354, Validation Loss: 1.0702, Alpha: 0.0000
Epoch: 8, Training Loss: 1.3521, Validation Loss: 0.8763, Alpha: 0.0000
Epoch: 9, Training Loss: 1.1760, Validation Loss: 0.8866, Alpha: 0.0000
Epoch: 10, Training Loss: 1.1242, Validation Loss: 0.6922, Alpha: 0.0000
Epoch: 11, Training Loss: 1.0675, Validation Loss: 0.5960, Alpha: 0.0000
Epoch: 12, Training Loss: 1.0086, Validation Loss: 0.6369, Alpha: 0.0000
Epoch: 13, Training Loss: 0.8368, Validation Loss: 0.5146, Alpha: 0.0000
Epoch: 14, Training Loss: 0.8063, Validation Loss: 0.4350, Alpha: 0.0000
Epoch: 15, Training Loss: 0.7770, Validation Loss: 0.4530, Alpha: 0.0000
Epoch: 16, Training Loss: 0.7209, Validation Loss: 0.6421, Alpha: 0.0000
Epoch: 17, Training Loss: 0.6390, Validation Loss: 0.4078, Alpha: 0.0000
Epoch: 18, Training Loss: 0.7028, Validation Loss: 0.3396, Alpha: 0.0000
Epoch: 19, Training Loss: 0.6502, Validation Loss: 0.3723, Alpha: 0.0000
Epoch: 20, Training Loss: 0.6560, Validation Loss: 0.3601, Alpha: 0.0000
Epoch: 21, Training Loss: 0.5846, Validation Loss: 0.3129, Alpha: 0.0000
Epoch: 22, Training Loss: 0.5450, Validation Loss: 0.2400, Alpha: 0.0000
Epoch: 23, Training Loss: 0.4912, Validation Loss: 0.3527, Alpha: 0.0000
Epoch: 24, Training Loss: 0.5203, Validation Loss: 0.2747, Alpha: 0.0000
Epoch: 25, Training Loss: 0.4075, Validation Loss: 0.2540, Alpha: 0.0000
Epoch: 26, Training Loss: 0.4376, Validation Loss: 0.3949, Alpha: 0.0000
Epoch: 27, Training Loss: 0.4926, Validation Loss: 0.2682, Alpha: 0.0000
Epoch: 28, Training Loss: 0.4903, Validation Loss: 0.2434, Alpha: 0.0000
Epoch: 29, Training Loss: 0.4252, Validation Loss: 0.2222, Alpha: 0.0000
Epoch: 30, Training Loss: 0.3687, Validation Loss: 0.2465, Alpha: 0.0000
Epoch: 31, Training Loss: 0.3957, Validation Loss: 0.2297, Alpha: 0.0000
Epoch: 32, Training Loss: 0.4475, Validation Loss: 0.2125, Alpha: 0.0000
Epoch: 33, Training Loss: 0.3168, Validation Loss: 0.3981, Alpha: 0.0000
Epoch: 34, Training Loss: 0.3981, Validation Loss: 0.2134, Alpha: 0.0000
Epoch: 35, Training Loss: 0.3697, Validation Loss: 0.1917, Alpha: 0.0000
Epoch: 36, Training Loss: 0.3316, Validation Loss: 0.1972, Alpha: 0.0000
Epoch: 37, Training Loss: 0.3114, Validation Loss: 0.1735, Alpha: 0.0000
Epoch: 38, Training Loss: 0.2827, Validation Loss: 0.1536, Alpha: 0.0000
Epoch: 39, Training Loss: 0.3495, Validation Loss: 0.1690, Alpha: 0.0000
Epoch: 40, Training Loss: 0.2537, Validation Loss: 0.1635, Alpha: 0.0000
Epoch: 41, Training Loss: 0.3181, Validation Loss: 0.2113, Alpha: 0.0000
Epoch: 42, Training Loss: 0.3107, Validation Loss: 0.2186, Alpha: 0.0000
Epoch: 43, Training Loss: 0.2858, Validation Loss: 0.3026, Alpha: 0.0000
Epoch: 44, Training Loss: 0.2776, Validation Loss: 0.1638, Alpha: 0.0000
Epoch: 45, Training Loss: 0.2482, Validation Loss: 0.1681, Alpha: 0.0000
Epoch: 46, Training Loss: 0.2726, Validation Loss: 0.1658, Alpha: 0.0000
Epoch: 47, Training Loss: 0.2874, Validation Loss: 0.1925, Alpha: 0.0000
Epoch: 48, Training Loss: 0.3023, Validation Loss: 0.1481, Alpha: 0.0000
Epoch: 49, Training Loss: 0.2763, Validation Loss: 0.1383, Alpha: 0.0000
Epoch: 50, Training Loss: 0.2628, Validation Loss: 0.1799, Alpha: 0.0000
Epoch: 51, Training Loss: 0.3125, Validation Loss: 0.1951, Alpha: 0.0000
Epoch: 52, Training Loss: 0.2704, Validation Loss: 0.1385, Alpha: 0.0000
Epoch: 53, Training Loss: 0.2175, Validation Loss: 0.1149, Alpha: 0.0000
Epoch: 54, Training Loss: 0.1949, Validation Loss: 0.1846, Alpha: 0.0000
Epoch: 55, Training Loss: 0.2446, Validation Loss: 0.1225, Alpha: 0.0000
Epoch: 56, Training Loss: 0.2209, Validation Loss: 0.1408, Alpha: 0.0000
Epoch: 57, Training Loss: 0.2387, Validation Loss: 0.1390, Alpha: 0.0000
Epoch: 58, Training Loss: 0.1838, Validation Loss: 0.1353, Alpha: 0.0000
Epoch: 59, Training Loss: 0.1666, Validation Loss: 0.1270, Alpha: 0.0000
Epoch: 60, Training Loss: 0.1607, Validation Loss: 0.1544, Alpha: 0.0000
Epoch: 61, Training Loss: 0.1996, Validation Loss: 0.1479, Alpha: 0.0000
Epoch: 62, Training Loss: 0.1722, Validation Loss: 0.1377, Alpha: 0.0000
Epoch: 63, Training Loss: 0.2126, Validation Loss: 0.1161, Alpha: 0.0000
Epoch: 64, Training Loss: 0.2365, Validation Loss: 0.1637, Alpha: 0.0000
Epoch: 65, Training Loss: 0.1653, Validation Loss: 0.1161, Alpha: 0.0000
Epoch: 66, Training Loss: 0.1006, Validation Loss: 0.0976, Alpha: 0.0000
Epoch: 67, Training Loss: 0.0874, Validation Loss: 0.0982, Alpha: 0.0000
Epoch: 68, Training Loss: 0.0944, Validation Loss: 0.1002, Alpha: 0.0000
Epoch: 69, Training Loss: 0.0819, Validation Loss: 0.0945, Alpha: 0.0000
Epoch: 70, Training Loss: 0.0605, Validation Loss: 0.0949, Alpha: 0.0000
Epoch: 71, Training Loss: 0.0744, Validation Loss: 0.0875, Alpha: 0.0000
Epoch: 72, Training Loss: 0.0795, Validation Loss: 0.0842, Alpha: 0.0000
Epoch: 73, Training Loss: 0.0932, Validation Loss: 0.0912, Alpha: 0.0000
Epoch: 74, Training Loss: 0.1089, Validation Loss: 0.0896, Alpha: 0.0000
Epoch: 75, Training Loss: 0.0857, Validation Loss: 0.0845, Alpha: 0.0000
Epoch: 76, Training Loss: 0.0636, Validation Loss: 0.0897, Alpha: 0.0000
Epoch: 77, Training Loss: 0.0720, Validation Loss: 0.0916, Alpha: 0.0000
Epoch: 78, Training Loss: 0.0709, Validation Loss: 0.0965, Alpha: 0.0000
Epoch: 79, Training Loss: 0.0710, Validation Loss: 0.0936, Alpha: 0.0000
Epoch: 80, Training Loss: 0.0599, Validation Loss: 0.0933, Alpha: 0.0000
Epoch: 81, Training Loss: 0.0776, Validation Loss: 0.0823, Alpha: 0.0000
Epoch: 82, Training Loss: 0.0506, Validation Loss: 0.0803, Alpha: 0.0000
Epoch: 83, Training Loss: 0.0655, Validation Loss: 0.0846, Alpha: 0.0000
Epoch: 84, Training Loss: 0.0545, Validation Loss: 0.0909, Alpha: 0.0000
Epoch: 85, Training Loss: 0.0568, Validation Loss: 0.0898, Alpha: 0.0000
Epoch: 86, Training Loss: 0.0610, Validation Loss: 0.0883, Alpha: 0.0000
Epoch: 87, Training Loss: 0.0543, Validation Loss: 0.0907, Alpha: 0.0000
Epoch: 88, Training Loss: 0.0562, Validation Loss: 0.0848, Alpha: 0.0000
Epoch: 89, Training Loss: 0.0533, Validation Loss: 0.0847, Alpha: 0.0000
Epoch: 90, Training Loss: 0.0543, Validation Loss: 0.0907, Alpha: 0.0000
Epoch: 91, Training Loss: 0.0596, Validation Loss: 0.0862, Alpha: 0.0000
Epoch: 92, Training Loss: 0.0567, Validation Loss: 0.0932, Alpha: 0.0000
Epoch: 93, Training Loss: 0.0601, Validation Loss: 0.0978, Alpha: 0.0000
Epoch: 94, Training Loss: 0.0537, Validation Loss: 0.0940, Alpha: 0.0000
Epoch: 95, Training Loss: 0.0488, Validation Loss: 0.0866, Alpha: 0.0000
Epoch: 96, Training Loss: 0.0563, Validation Loss: 0.0887, Alpha: 0.0000
Epoch: 97, Training Loss: 0.0369, Validation Loss: 0.0891, Alpha: 0.0000
Epoch: 98, Training Loss: 0.0414, Validation Loss: 0.0879, Alpha: 0.0000
Epoch: 99, Training Loss: 0.0352, Validation Loss: 0.0903, Alpha: 0.0000
Epoch: 100, Training Loss: 0.0565, Validation Loss: 0.0877, Alpha: 0.0000
Running increased_features_complexity classification training with alpha = 0.2 using script scripts/increased_features_complexity/hierarchical-classification-training.py
/home/bcw/gits/FjordVision/fjordvision/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
Epoch: 1, Training Loss: 4.9648, Validation Loss: 4.8470, Alpha: 0.2000
Epoch: 2, Training Loss: 4.1494, Validation Loss: 4.0500, Alpha: 0.2000
Epoch: 3, Training Loss: 3.6489, Validation Loss: 3.9974, Alpha: 0.2000
Epoch: 4, Training Loss: 3.2473, Validation Loss: 2.9639, Alpha: 0.2000
Epoch: 5, Training Loss: 3.0112, Validation Loss: 2.8356, Alpha: 0.2000
Epoch: 6, Training Loss: 2.8176, Validation Loss: 2.7273, Alpha: 0.2000
Epoch: 7, Training Loss: 2.6712, Validation Loss: 2.4495, Alpha: 0.2000
Epoch: 8, Training Loss: 2.3967, Validation Loss: 2.4693, Alpha: 0.2000
Epoch: 9, Training Loss: 2.1792, Validation Loss: 1.7970, Alpha: 0.2000
Epoch: 10, Training Loss: 1.9429, Validation Loss: 1.7259, Alpha: 0.2000
Epoch: 11, Training Loss: 1.7061, Validation Loss: 1.3731, Alpha: 0.2000
Epoch: 12, Training Loss: 1.5791, Validation Loss: 1.1017, Alpha: 0.2000
Epoch: 13, Training Loss: 1.4501, Validation Loss: 1.1118, Alpha: 0.2000
Epoch: 14, Training Loss: 1.2212, Validation Loss: 1.0473, Alpha: 0.2000
Epoch: 15, Training Loss: 1.2098, Validation Loss: 1.0178, Alpha: 0.2000
Epoch: 16, Training Loss: 1.0629, Validation Loss: 0.7788, Alpha: 0.2000
Epoch: 17, Training Loss: 1.0057, Validation Loss: 0.7508, Alpha: 0.2000
Epoch: 18, Training Loss: 0.9337, Validation Loss: 0.7854, Alpha: 0.2000
Epoch: 19, Training Loss: 0.7960, Validation Loss: 0.6110, Alpha: 0.2000
Epoch: 20, Training Loss: 0.7810, Validation Loss: 0.5700, Alpha: 0.2000
Epoch: 21, Training Loss: 0.6582, Validation Loss: 0.5574, Alpha: 0.2000
Epoch: 22, Training Loss: 0.6760, Validation Loss: 0.4528, Alpha: 0.2000
Epoch: 23, Training Loss: 0.6114, Validation Loss: 0.3988, Alpha: 0.2000
Epoch: 24, Training Loss: 0.5927, Validation Loss: 0.3394, Alpha: 0.2000
Epoch: 25, Training Loss: 0.5173, Validation Loss: 0.5025, Alpha: 0.2000
Epoch: 26, Training Loss: 0.5393, Validation Loss: 0.4323, Alpha: 0.2000
Epoch: 27, Training Loss: 0.5129, Validation Loss: 0.3331, Alpha: 0.2000
Epoch: 28, Training Loss: 0.4611, Validation Loss: 0.3816, Alpha: 0.2000
Epoch: 29, Training Loss: 0.4358, Validation Loss: 0.3811, Alpha: 0.2000
Epoch: 30, Training Loss: 0.4168, Validation Loss: 0.3217, Alpha: 0.2000
Epoch: 31, Training Loss: 0.4255, Validation Loss: 0.2632, Alpha: 0.2000
Epoch: 32, Training Loss: 0.3388, Validation Loss: 0.2475, Alpha: 0.2000
Epoch: 33, Training Loss: 0.3629, Validation Loss: 0.2592, Alpha: 0.2000
Epoch: 34, Training Loss: 0.3411, Validation Loss: 0.2401, Alpha: 0.2000
Epoch: 35, Training Loss: 0.3219, Validation Loss: 0.3365, Alpha: 0.2000
Epoch: 36, Training Loss: 0.3115, Validation Loss: 0.3316, Alpha: 0.2000
Epoch: 37, Training Loss: 0.2821, Validation Loss: 0.2675, Alpha: 0.2000
Epoch: 38, Training Loss: 0.2582, Validation Loss: 0.2367, Alpha: 0.2000
Epoch: 39, Training Loss: 0.2706, Validation Loss: 0.2431, Alpha: 0.2000
Epoch: 40, Training Loss: 0.2864, Validation Loss: 0.2990, Alpha: 0.2000
Epoch: 41, Training Loss: 0.3364, Validation Loss: 0.3039, Alpha: 0.2000
Epoch: 42, Training Loss: 0.3476, Validation Loss: 0.2745, Alpha: 0.2000
Epoch: 43, Training Loss: 0.3004, Validation Loss: 0.3047, Alpha: 0.2000
Epoch: 44, Training Loss: 0.2500, Validation Loss: 0.3066, Alpha: 0.2000
Epoch: 45, Training Loss: 0.2690, Validation Loss: 0.2242, Alpha: 0.2000
Epoch: 46, Training Loss: 0.2712, Validation Loss: 0.2111, Alpha: 0.2000
Epoch: 47, Training Loss: 0.2372, Validation Loss: 0.2043, Alpha: 0.2000
Epoch: 48, Training Loss: 0.2369, Validation Loss: 0.3165, Alpha: 0.2000
Epoch: 49, Training Loss: 0.2242, Validation Loss: 0.1855, Alpha: 0.2000
Epoch: 50, Training Loss: 0.2070, Validation Loss: 0.1968, Alpha: 0.2000
Epoch: 51, Training Loss: 0.2278, Validation Loss: 0.1789, Alpha: 0.2000
Epoch: 52, Training Loss: 0.2376, Validation Loss: 0.2520, Alpha: 0.2000
Epoch: 53, Training Loss: 0.1770, Validation Loss: 0.2138, Alpha: 0.2000
Epoch: 54, Training Loss: 0.2387, Validation Loss: 0.2179, Alpha: 0.2000
Epoch: 55, Training Loss: 0.2384, Validation Loss: 0.2554, Alpha: 0.2000
Epoch: 56, Training Loss: 0.2066, Validation Loss: 0.2363, Alpha: 0.2000
Epoch: 57, Training Loss: 0.1708, Validation Loss: 0.2153, Alpha: 0.2000
Epoch: 58, Training Loss: 0.1990, Validation Loss: 0.2269, Alpha: 0.2000
Epoch: 59, Training Loss: 0.1361, Validation Loss: 0.1719, Alpha: 0.2000
Epoch: 60, Training Loss: 0.1376, Validation Loss: 0.2106, Alpha: 0.2000
Epoch: 61, Training Loss: 0.1532, Validation Loss: 0.1882, Alpha: 0.2000
Epoch: 62, Training Loss: 0.1930, Validation Loss: 0.2018, Alpha: 0.2000
Epoch: 63, Training Loss: 0.1510, Validation Loss: 0.1992, Alpha: 0.2000
Epoch: 64, Training Loss: 0.1449, Validation Loss: 0.2047, Alpha: 0.2000
Epoch: 65, Training Loss: 0.1593, Validation Loss: 0.1804, Alpha: 0.2000
Epoch: 66, Training Loss: 0.1228, Validation Loss: 0.1795, Alpha: 0.2000
Epoch: 67, Training Loss: 0.1401, Validation Loss: 0.1711, Alpha: 0.2000
Epoch: 68, Training Loss: 0.1196, Validation Loss: 0.2613, Alpha: 0.2000
Epoch: 69, Training Loss: 0.1415, Validation Loss: 0.2262, Alpha: 0.2000
Epoch: 70, Training Loss: 0.1816, Validation Loss: 0.1473, Alpha: 0.2000
Epoch: 71, Training Loss: 0.1160, Validation Loss: 0.1866, Alpha: 0.2000
Epoch: 72, Training Loss: 0.1384, Validation Loss: 0.1698, Alpha: 0.2000
Epoch: 73, Training Loss: 0.0971, Validation Loss: 0.2358, Alpha: 0.2000
Epoch: 74, Training Loss: 0.1368, Validation Loss: 0.1608, Alpha: 0.2000
Epoch: 75, Training Loss: 0.1190, Validation Loss: 0.2151, Alpha: 0.2000
Epoch: 76, Training Loss: 0.1522, Validation Loss: 0.1871, Alpha: 0.2000
Epoch: 77, Training Loss: 0.1041, Validation Loss: 0.1880, Alpha: 0.2000
Epoch: 78, Training Loss: 0.1598, Validation Loss: 0.2371, Alpha: 0.2000
Epoch: 79, Training Loss: 0.1642, Validation Loss: 0.2056, Alpha: 0.2000
Epoch: 80, Training Loss: 0.1009, Validation Loss: 0.2808, Alpha: 0.2000
Epoch: 81, Training Loss: 0.1074, Validation Loss: 0.2252, Alpha: 0.2000
Epoch: 82, Training Loss: 0.1196, Validation Loss: 0.1521, Alpha: 0.2000
Epoch: 83, Training Loss: 0.1025, Validation Loss: 0.1301, Alpha: 0.2000
Epoch: 84, Training Loss: 0.0542, Validation Loss: 0.1269, Alpha: 0.2000
Epoch: 85, Training Loss: 0.0477, Validation Loss: 0.1428, Alpha: 0.2000
Epoch: 86, Training Loss: 0.0384, Validation Loss: 0.1326, Alpha: 0.2000
Epoch: 87, Training Loss: 0.0494, Validation Loss: 0.1239, Alpha: 0.2000
Epoch: 88, Training Loss: 0.0296, Validation Loss: 0.1262, Alpha: 0.2000
Epoch: 89, Training Loss: 0.0342, Validation Loss: 0.1235, Alpha: 0.2000
Epoch: 90, Training Loss: 0.0396, Validation Loss: 0.1130, Alpha: 0.2000
Epoch: 91, Training Loss: 0.0247, Validation Loss: 0.1196, Alpha: 0.2000
Epoch: 92, Training Loss: 0.0257, Validation Loss: 0.1135, Alpha: 0.2000
Epoch: 93, Training Loss: 0.0311, Validation Loss: 0.1195, Alpha: 0.2000
Epoch: 94, Training Loss: 0.0311, Validation Loss: 0.1263, Alpha: 0.2000
Epoch: 95, Training Loss: 0.0383, Validation Loss: 0.1239, Alpha: 0.2000
Epoch: 96, Training Loss: 0.0177, Validation Loss: 0.1231, Alpha: 0.2000
Epoch: 97, Training Loss: 0.0256, Validation Loss: 0.1197, Alpha: 0.2000
Epoch: 98, Training Loss: 0.0224, Validation Loss: 0.1247, Alpha: 0.2000
Epoch: 99, Training Loss: 0.0201, Validation Loss: 0.1251, Alpha: 0.2000
Epoch: 100, Training Loss: 0.0242, Validation Loss: 0.1321, Alpha: 0.2000
Running increased_features_complexity classification training with alpha = 0.5 using script scripts/increased_features_complexity/hierarchical-classification-training.py
/home/bcw/gits/FjordVision/fjordvision/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
Epoch: 1, Training Loss: 5.0539, Validation Loss: 4.3901, Alpha: 0.5000
Epoch: 2, Training Loss: 4.1200, Validation Loss: 4.2349, Alpha: 0.5000
Epoch: 3, Training Loss: 3.5331, Validation Loss: 3.1272, Alpha: 0.5000
Epoch: 4, Training Loss: 3.1358, Validation Loss: 2.7428, Alpha: 0.5000
Epoch: 5, Training Loss: 2.8378, Validation Loss: 2.4643, Alpha: 0.5000
Epoch: 6, Training Loss: 2.4918, Validation Loss: 2.2727, Alpha: 0.5000
Epoch: 7, Training Loss: 2.1875, Validation Loss: 1.8602, Alpha: 0.5000
Epoch: 8, Training Loss: 1.9118, Validation Loss: 1.5537, Alpha: 0.5000
Epoch: 9, Training Loss: 1.6558, Validation Loss: 1.1490, Alpha: 0.5000
Epoch: 10, Training Loss: 1.4634, Validation Loss: 1.1381, Alpha: 0.5000
Epoch: 11, Training Loss: 1.3843, Validation Loss: 0.8254, Alpha: 0.5000
Epoch: 12, Training Loss: 1.1579, Validation Loss: 0.8394, Alpha: 0.5000
Epoch: 13, Training Loss: 1.0917, Validation Loss: 1.0914, Alpha: 0.5000
Epoch: 14, Training Loss: 1.0238, Validation Loss: 0.6457, Alpha: 0.5000
Epoch: 15, Training Loss: 0.9126, Validation Loss: 0.6067, Alpha: 0.5000
Epoch: 16, Training Loss: 0.8919, Validation Loss: 0.6229, Alpha: 0.5000
Epoch: 17, Training Loss: 0.8871, Validation Loss: 0.5990, Alpha: 0.5000
Epoch: 18, Training Loss: 0.7885, Validation Loss: 0.5781, Alpha: 0.5000
Epoch: 19, Training Loss: 0.7483, Validation Loss: 0.4536, Alpha: 0.5000
Epoch: 20, Training Loss: 0.7358, Validation Loss: 0.5498, Alpha: 0.5000
Epoch: 21, Training Loss: 0.7547, Validation Loss: 0.4589, Alpha: 0.5000
Epoch: 22, Training Loss: 0.6392, Validation Loss: 0.3967, Alpha: 0.5000
Epoch: 23, Training Loss: 0.6023, Validation Loss: 0.4589, Alpha: 0.5000
Epoch: 24, Training Loss: 0.5862, Validation Loss: 0.3635, Alpha: 0.5000
Epoch: 25, Training Loss: 0.5868, Validation Loss: 0.3356, Alpha: 0.5000
Epoch: 26, Training Loss: 0.5512, Validation Loss: 0.3071, Alpha: 0.5000
Epoch: 27, Training Loss: 0.4620, Validation Loss: 0.2678, Alpha: 0.5000
Epoch: 28, Training Loss: 0.4413, Validation Loss: 0.3029, Alpha: 0.5000
Epoch: 29, Training Loss: 0.4571, Validation Loss: 0.2188, Alpha: 0.5000
Epoch: 30, Training Loss: 0.4612, Validation Loss: 0.2700, Alpha: 0.5000
Epoch: 31, Training Loss: 0.3905, Validation Loss: 0.2390, Alpha: 0.5000
Epoch: 32, Training Loss: 0.4186, Validation Loss: 0.2173, Alpha: 0.5000
Epoch: 33, Training Loss: 0.3417, Validation Loss: 0.2330, Alpha: 0.5000
Epoch: 34, Training Loss: 0.3807, Validation Loss: 0.3030, Alpha: 0.5000
Epoch: 35, Training Loss: 0.3713, Validation Loss: 0.2022, Alpha: 0.5000
Epoch: 36, Training Loss: 0.3713, Validation Loss: 0.2007, Alpha: 0.5000
Epoch: 37, Training Loss: 0.3588, Validation Loss: 0.2177, Alpha: 0.5000
Epoch: 38, Training Loss: 0.3371, Validation Loss: 0.1866, Alpha: 0.5000
Epoch: 39, Training Loss: 0.3674, Validation Loss: 0.2274, Alpha: 0.5000
Epoch: 40, Training Loss: 0.3715, Validation Loss: 0.2247, Alpha: 0.5000
Epoch: 41, Training Loss: 0.3971, Validation Loss: 0.1947, Alpha: 0.5000
Epoch: 42, Training Loss: 0.3025, Validation Loss: 0.2317, Alpha: 0.5000
Epoch: 43, Training Loss: 0.2679, Validation Loss: 0.2143, Alpha: 0.5000
Epoch: 44, Training Loss: 0.3691, Validation Loss: 0.2910, Alpha: 0.5000
Epoch: 45, Training Loss: 0.2787, Validation Loss: 0.1467, Alpha: 0.5000
Epoch: 46, Training Loss: 0.2703, Validation Loss: 0.1870, Alpha: 0.5000
Epoch: 47, Training Loss: 0.3819, Validation Loss: 0.2080, Alpha: 0.5000
Epoch: 48, Training Loss: 0.2960, Validation Loss: 0.1969, Alpha: 0.5000
Epoch: 49, Training Loss: 0.2732, Validation Loss: 0.1192, Alpha: 0.5000
Epoch: 50, Training Loss: 0.2410, Validation Loss: 0.2019, Alpha: 0.5000
Epoch: 51, Training Loss: 0.2423, Validation Loss: 0.1780, Alpha: 0.5000
Epoch: 52, Training Loss: 0.2376, Validation Loss: 0.1599, Alpha: 0.5000
Epoch: 53, Training Loss: 0.2640, Validation Loss: 0.1876, Alpha: 0.5000
Epoch: 54, Training Loss: 0.2595, Validation Loss: 0.1719, Alpha: 0.5000
Epoch: 55, Training Loss: 0.2894, Validation Loss: 0.1893, Alpha: 0.5000
Epoch: 56, Training Loss: 0.2513, Validation Loss: 0.1813, Alpha: 0.5000
Epoch: 57, Training Loss: 0.2805, Validation Loss: 0.1730, Alpha: 0.5000
Epoch: 58, Training Loss: 0.2207, Validation Loss: 0.1625, Alpha: 0.5000
Epoch: 59, Training Loss: 0.1908, Validation Loss: 0.1468, Alpha: 0.5000
Epoch: 60, Training Loss: 0.2079, Validation Loss: 0.1561, Alpha: 0.5000
Epoch: 61, Training Loss: 0.1394, Validation Loss: 0.1157, Alpha: 0.5000
Epoch: 62, Training Loss: 0.1397, Validation Loss: 0.1046, Alpha: 0.5000
Epoch: 63, Training Loss: 0.1179, Validation Loss: 0.0963, Alpha: 0.5000
Epoch: 64, Training Loss: 0.1306, Validation Loss: 0.0876, Alpha: 0.5000
Epoch: 65, Training Loss: 0.0904, Validation Loss: 0.0873, Alpha: 0.5000
Epoch: 66, Training Loss: 0.1287, Validation Loss: 0.0925, Alpha: 0.5000
Epoch: 67, Training Loss: 0.1091, Validation Loss: 0.0944, Alpha: 0.5000
Epoch: 68, Training Loss: 0.0812, Validation Loss: 0.0923, Alpha: 0.5000
Epoch: 69, Training Loss: 0.0932, Validation Loss: 0.0939, Alpha: 0.5000
Epoch: 70, Training Loss: 0.0709, Validation Loss: 0.0886, Alpha: 0.5000
Epoch: 71, Training Loss: 0.0721, Validation Loss: 0.0934, Alpha: 0.5000
Epoch: 72, Training Loss: 0.1030, Validation Loss: 0.0894, Alpha: 0.5000
Epoch: 73, Training Loss: 0.0889, Validation Loss: 0.0998, Alpha: 0.5000
Epoch: 74, Training Loss: 0.1007, Validation Loss: 0.0993, Alpha: 0.5000
Epoch: 75, Training Loss: 0.0671, Validation Loss: 0.0983, Alpha: 0.5000
Epoch: 76, Training Loss: 0.0864, Validation Loss: 0.0927, Alpha: 0.5000
Epoch: 77, Training Loss: 0.0787, Validation Loss: 0.0932, Alpha: 0.5000
Epoch: 78, Training Loss: 0.0827, Validation Loss: 0.0981, Alpha: 0.5000
Epoch: 79, Training Loss: 0.0637, Validation Loss: 0.0932, Alpha: 0.5000
Epoch: 80, Training Loss: 0.0536, Validation Loss: 0.0926, Alpha: 0.5000
Epoch: 81, Training Loss: 0.0721, Validation Loss: 0.0957, Alpha: 0.5000
Epoch: 82, Training Loss: 0.0526, Validation Loss: 0.0972, Alpha: 0.5000
Epoch: 83, Training Loss: 0.0764, Validation Loss: 0.0920, Alpha: 0.5000
Epoch: 84, Training Loss: 0.0546, Validation Loss: 0.0924, Alpha: 0.5000
Epoch: 85, Training Loss: 0.0776, Validation Loss: 0.0932, Alpha: 0.5000
Early stopping triggered.
Running increased_features_complexity classification training with alpha = 0.8 using script scripts/increased_features_complexity/hierarchical-classification-training.py
/home/bcw/gits/FjordVision/fjordvision/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
Epoch: 1, Training Loss: 4.7004, Validation Loss: 4.0678, Alpha: 0.8000
Epoch: 2, Training Loss: 3.3856, Validation Loss: 2.5083, Alpha: 0.8000
Epoch: 3, Training Loss: 2.5693, Validation Loss: 2.0048, Alpha: 0.8000
Epoch: 4, Training Loss: 2.0652, Validation Loss: 1.6875, Alpha: 0.8000
Epoch: 5, Training Loss: 1.7123, Validation Loss: 1.4119, Alpha: 0.8000
Epoch: 6, Training Loss: 1.4229, Validation Loss: 0.9591, Alpha: 0.8000
Epoch: 7, Training Loss: 1.2231, Validation Loss: 1.1232, Alpha: 0.8000
Epoch: 8, Training Loss: 1.0577, Validation Loss: 0.6342, Alpha: 0.8000
Epoch: 9, Training Loss: 0.8969, Validation Loss: 0.7065, Alpha: 0.8000
Epoch: 10, Training Loss: 0.8484, Validation Loss: 0.4787, Alpha: 0.8000
Epoch: 11, Training Loss: 0.6965, Validation Loss: 0.5248, Alpha: 0.8000
Epoch: 12, Training Loss: 0.6572, Validation Loss: 0.5210, Alpha: 0.8000
Epoch: 13, Training Loss: 0.6128, Validation Loss: 0.3513, Alpha: 0.8000
Epoch: 14, Training Loss: 0.5577, Validation Loss: 0.3693, Alpha: 0.8000
Epoch: 15, Training Loss: 0.5510, Validation Loss: 0.3344, Alpha: 0.8000
Epoch: 16, Training Loss: 0.5126, Validation Loss: 0.4543, Alpha: 0.8000
Epoch: 17, Training Loss: 0.4594, Validation Loss: 0.3522, Alpha: 0.8000
Epoch: 18, Training Loss: 0.5436, Validation Loss: 0.2876, Alpha: 0.8000
Epoch: 19, Training Loss: 0.4658, Validation Loss: 0.3391, Alpha: 0.8000
Epoch: 20, Training Loss: 0.3978, Validation Loss: 0.2988, Alpha: 0.8000
Epoch: 21, Training Loss: 0.3792, Validation Loss: 0.2434, Alpha: 0.8000
Epoch: 22, Training Loss: 0.3872, Validation Loss: 0.2394, Alpha: 0.8000
Epoch: 23, Training Loss: 0.3353, Validation Loss: 0.2162, Alpha: 0.8000
Epoch: 24, Training Loss: 0.3666, Validation Loss: 0.3138, Alpha: 0.8000
Epoch: 25, Training Loss: 0.3296, Validation Loss: 0.3418, Alpha: 0.8000
Epoch: 26, Training Loss: 0.4154, Validation Loss: 0.1939, Alpha: 0.8000
Epoch: 27, Training Loss: 0.3026, Validation Loss: 0.1773, Alpha: 0.8000
Epoch: 28, Training Loss: 0.3121, Validation Loss: 0.2911, Alpha: 0.8000
Epoch: 29, Training Loss: 0.3016, Validation Loss: 0.1704, Alpha: 0.8000
Epoch: 30, Training Loss: 0.2914, Validation Loss: 0.1830, Alpha: 0.8000
Epoch: 31, Training Loss: 0.2411, Validation Loss: 0.1978, Alpha: 0.8000
Epoch: 32, Training Loss: 0.2513, Validation Loss: 0.2460, Alpha: 0.8000
Epoch: 33, Training Loss: 0.2221, Validation Loss: 0.1871, Alpha: 0.8000
Epoch: 34, Training Loss: 0.2347, Validation Loss: 0.1660, Alpha: 0.8000
Epoch: 35, Training Loss: 0.2382, Validation Loss: 0.1645, Alpha: 0.8000
Epoch: 36, Training Loss: 0.1806, Validation Loss: 0.1885, Alpha: 0.8000
Epoch: 37, Training Loss: 0.1838, Validation Loss: 0.2059, Alpha: 0.8000
Epoch: 38, Training Loss: 0.2591, Validation Loss: 0.2574, Alpha: 0.8000
Epoch: 39, Training Loss: 0.2231, Validation Loss: 0.1478, Alpha: 0.8000
Epoch: 40, Training Loss: 0.2445, Validation Loss: 0.1639, Alpha: 0.8000
Epoch: 41, Training Loss: 0.2338, Validation Loss: 0.3164, Alpha: 0.8000
Epoch: 42, Training Loss: 0.1552, Validation Loss: 0.4109, Alpha: 0.8000
Epoch: 43, Training Loss: 0.2213, Validation Loss: 0.1148, Alpha: 0.8000
Epoch: 44, Training Loss: 0.1772, Validation Loss: 0.1618, Alpha: 0.8000
Epoch: 45, Training Loss: 0.1883, Validation Loss: 0.2360, Alpha: 0.8000
Epoch: 46, Training Loss: 0.2007, Validation Loss: 0.1522, Alpha: 0.8000
Epoch: 47, Training Loss: 0.1945, Validation Loss: 0.1884, Alpha: 0.8000
Epoch: 48, Training Loss: 0.1635, Validation Loss: 0.1341, Alpha: 0.8000
Epoch: 49, Training Loss: 0.1936, Validation Loss: 0.2150, Alpha: 0.8000
Epoch: 50, Training Loss: 0.1345, Validation Loss: 0.2136, Alpha: 0.8000
Epoch: 51, Training Loss: 0.1827, Validation Loss: 0.1381, Alpha: 0.8000
Epoch: 52, Training Loss: 0.1251, Validation Loss: 0.1543, Alpha: 0.8000
Epoch: 53, Training Loss: 0.1615, Validation Loss: 0.1663, Alpha: 0.8000
Epoch: 54, Training Loss: 0.1462, Validation Loss: 0.1446, Alpha: 0.8000
Epoch: 55, Training Loss: 0.1324, Validation Loss: 0.1378, Alpha: 0.8000
Epoch: 56, Training Loss: 0.0921, Validation Loss: 0.1176, Alpha: 0.8000
Epoch: 57, Training Loss: 0.0894, Validation Loss: 0.1090, Alpha: 0.8000
Epoch: 58, Training Loss: 0.0774, Validation Loss: 0.1124, Alpha: 0.8000
Epoch: 59, Training Loss: 0.0740, Validation Loss: 0.1041, Alpha: 0.8000
Epoch: 60, Training Loss: 0.0579, Validation Loss: 0.1041, Alpha: 0.8000
Epoch: 61, Training Loss: 0.0705, Validation Loss: 0.1073, Alpha: 0.8000
Epoch: 62, Training Loss: 0.0568, Validation Loss: 0.0968, Alpha: 0.8000
Epoch: 63, Training Loss: 0.0515, Validation Loss: 0.1017, Alpha: 0.8000
Epoch: 64, Training Loss: 0.0639, Validation Loss: 0.0959, Alpha: 0.8000
Epoch: 65, Training Loss: 0.0438, Validation Loss: 0.0984, Alpha: 0.8000
Epoch: 66, Training Loss: 0.0816, Validation Loss: 0.1009, Alpha: 0.8000
Epoch: 67, Training Loss: 0.0336, Validation Loss: 0.1050, Alpha: 0.8000
Epoch: 68, Training Loss: 0.0448, Validation Loss: 0.0976, Alpha: 0.8000
Epoch: 69, Training Loss: 0.0452, Validation Loss: 0.1039, Alpha: 0.8000
Epoch: 70, Training Loss: 0.0376, Validation Loss: 0.1013, Alpha: 0.8000
Epoch: 71, Training Loss: 0.0330, Validation Loss: 0.1075, Alpha: 0.8000
Epoch: 72, Training Loss: 0.0409, Validation Loss: 0.1035, Alpha: 0.8000
Epoch: 73, Training Loss: 0.0403, Validation Loss: 0.1021, Alpha: 0.8000
Epoch: 74, Training Loss: 0.0312, Validation Loss: 0.1046, Alpha: 0.8000
Epoch: 75, Training Loss: 0.0312, Validation Loss: 0.0994, Alpha: 0.8000
Epoch: 76, Training Loss: 0.0347, Validation Loss: 0.0957, Alpha: 0.8000
Epoch: 77, Training Loss: 0.0299, Validation Loss: 0.0980, Alpha: 0.8000
Epoch: 78, Training Loss: 0.0480, Validation Loss: 0.1031, Alpha: 0.8000
Epoch: 79, Training Loss: 0.0456, Validation Loss: 0.1003, Alpha: 0.8000
Epoch: 80, Training Loss: 0.0352, Validation Loss: 0.1009, Alpha: 0.8000
Epoch: 81, Training Loss: 0.0477, Validation Loss: 0.0945, Alpha: 0.8000
Epoch: 82, Training Loss: 0.0353, Validation Loss: 0.0988, Alpha: 0.8000
Epoch: 83, Training Loss: 0.0305, Validation Loss: 0.0976, Alpha: 0.8000
Epoch: 84, Training Loss: 0.0269, Validation Loss: 0.0995, Alpha: 0.8000
Epoch: 85, Training Loss: 0.0382, Validation Loss: 0.1037, Alpha: 0.8000
Epoch: 86, Training Loss: 0.0454, Validation Loss: 0.1036, Alpha: 0.8000
Epoch: 87, Training Loss: 0.0368, Validation Loss: 0.0977, Alpha: 0.8000
Epoch: 88, Training Loss: 0.0276, Validation Loss: 0.0969, Alpha: 0.8000
Epoch: 89, Training Loss: 0.0271, Validation Loss: 0.1015, Alpha: 0.8000
Epoch: 90, Training Loss: 0.0272, Validation Loss: 0.1002, Alpha: 0.8000
Epoch: 91, Training Loss: 0.0363, Validation Loss: 0.1058, Alpha: 0.8000
Epoch: 92, Training Loss: 0.0279, Validation Loss: 0.1064, Alpha: 0.8000
Epoch: 93, Training Loss: 0.0316, Validation Loss: 0.1040, Alpha: 0.8000
Epoch: 94, Training Loss: 0.0310, Validation Loss: 0.1028, Alpha: 0.8000
Epoch: 95, Training Loss: 0.0242, Validation Loss: 0.1035, Alpha: 0.8000
Epoch: 96, Training Loss: 0.0328, Validation Loss: 0.0989, Alpha: 0.8000
Epoch: 97, Training Loss: 0.0401, Validation Loss: 0.1029, Alpha: 0.8000
Epoch: 98, Training Loss: 0.0321, Validation Loss: 0.1023, Alpha: 0.8000
Epoch: 99, Training Loss: 0.0298, Validation Loss: 0.1008, Alpha: 0.8000
Epoch: 100, Training Loss: 0.0398, Validation Loss: 0.0998, Alpha: 0.8000
Running increased_features_complexity classification training with alpha = 1 using script scripts/increased_features_complexity/hierarchical-classification-training.py
/home/bcw/gits/FjordVision/fjordvision/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
Epoch: 1, Training Loss: 4.8652, Validation Loss: 4.8744, Alpha: 1.0000
Epoch: 2, Training Loss: 4.0174, Validation Loss: 3.4573, Alpha: 1.0000
Epoch: 3, Training Loss: 3.4308, Validation Loss: 4.1932, Alpha: 1.0000
Epoch: 4, Training Loss: 3.1035, Validation Loss: 2.9886, Alpha: 1.0000
Epoch: 5, Training Loss: 2.8258, Validation Loss: 2.5628, Alpha: 1.0000
Epoch: 6, Training Loss: 2.5903, Validation Loss: 2.4580, Alpha: 1.0000
Epoch: 7, Training Loss: 2.3148, Validation Loss: 2.1088, Alpha: 1.0000
Epoch: 8, Training Loss: 1.9856, Validation Loss: 1.7679, Alpha: 1.0000
Epoch: 9, Training Loss: 1.7045, Validation Loss: 1.4783, Alpha: 1.0000
Epoch: 10, Training Loss: 1.6057, Validation Loss: 1.0978, Alpha: 1.0000
Epoch: 11, Training Loss: 1.4294, Validation Loss: 1.0240, Alpha: 1.0000
Epoch: 12, Training Loss: 1.2332, Validation Loss: 0.8027, Alpha: 1.0000
Epoch: 13, Training Loss: 1.1508, Validation Loss: 0.7168, Alpha: 1.0000
Epoch: 14, Training Loss: 1.0136, Validation Loss: 0.6321, Alpha: 1.0000
Epoch: 15, Training Loss: 0.8986, Validation Loss: 0.5980, Alpha: 1.0000
Epoch: 16, Training Loss: 0.7920, Validation Loss: 0.4777, Alpha: 1.0000
Epoch: 17, Training Loss: 0.7286, Validation Loss: 0.4458, Alpha: 1.0000
Epoch: 18, Training Loss: 0.7600, Validation Loss: 0.4354, Alpha: 1.0000
Epoch: 19, Training Loss: 0.7036, Validation Loss: 0.4339, Alpha: 1.0000
Epoch: 20, Training Loss: 0.7015, Validation Loss: 0.5739, Alpha: 1.0000
Epoch: 21, Training Loss: 0.6918, Validation Loss: 0.3989, Alpha: 1.0000
Epoch: 22, Training Loss: 0.6360, Validation Loss: 0.5007, Alpha: 1.0000
Epoch: 23, Training Loss: 0.5385, Validation Loss: 0.2873, Alpha: 1.0000
Epoch: 24, Training Loss: 0.4979, Validation Loss: 0.3910, Alpha: 1.0000
Epoch: 25, Training Loss: 0.5577, Validation Loss: 0.3041, Alpha: 1.0000
Epoch: 26, Training Loss: 0.4561, Validation Loss: 0.2911, Alpha: 1.0000
Epoch: 27, Training Loss: 0.4731, Validation Loss: 0.3194, Alpha: 1.0000
Epoch: 28, Training Loss: 0.4536, Validation Loss: 0.2814, Alpha: 1.0000
Epoch: 29, Training Loss: 0.4642, Validation Loss: 0.2376, Alpha: 1.0000
Epoch: 30, Training Loss: 0.4950, Validation Loss: 0.2647, Alpha: 1.0000
Epoch: 31, Training Loss: 0.4399, Validation Loss: 0.4204, Alpha: 1.0000
Epoch: 32, Training Loss: 0.3612, Validation Loss: 0.2904, Alpha: 1.0000
Epoch: 33, Training Loss: 0.3170, Validation Loss: 0.2323, Alpha: 1.0000
Epoch: 34, Training Loss: 0.3339, Validation Loss: 0.2159, Alpha: 1.0000
Epoch: 35, Training Loss: 0.3552, Validation Loss: 0.2408, Alpha: 1.0000
Epoch: 36, Training Loss: 0.3077, Validation Loss: 0.2017, Alpha: 1.0000
Epoch: 37, Training Loss: 0.3451, Validation Loss: 0.1882, Alpha: 1.0000
Epoch: 38, Training Loss: 0.3696, Validation Loss: 0.1669, Alpha: 1.0000
Epoch: 39, Training Loss: 0.2863, Validation Loss: 0.2141, Alpha: 1.0000
Epoch: 40, Training Loss: 0.3204, Validation Loss: 0.1793, Alpha: 1.0000
Epoch: 41, Training Loss: 0.2810, Validation Loss: 0.1753, Alpha: 1.0000
Epoch: 42, Training Loss: 0.3134, Validation Loss: 0.1649, Alpha: 1.0000
Epoch: 43, Training Loss: 0.2860, Validation Loss: 0.2738, Alpha: 1.0000
Epoch: 44, Training Loss: 0.2528, Validation Loss: 0.1790, Alpha: 1.0000
Epoch: 45, Training Loss: 0.2796, Validation Loss: 0.1313, Alpha: 1.0000
Epoch: 46, Training Loss: 0.2984, Validation Loss: 0.1627, Alpha: 1.0000
Epoch: 47, Training Loss: 0.2668, Validation Loss: 0.2260, Alpha: 1.0000
Epoch: 48, Training Loss: 0.2635, Validation Loss: 0.3933, Alpha: 1.0000
Epoch: 49, Training Loss: 0.2669, Validation Loss: 0.2482, Alpha: 1.0000
Epoch: 50, Training Loss: 0.2391, Validation Loss: 0.2102, Alpha: 1.0000
Epoch: 51, Training Loss: 0.2257, Validation Loss: 0.1756, Alpha: 1.0000
Epoch: 52, Training Loss: 0.1987, Validation Loss: 0.1483, Alpha: 1.0000
Epoch: 53, Training Loss: 0.2165, Validation Loss: 0.1659, Alpha: 1.0000
Epoch: 54, Training Loss: 0.2319, Validation Loss: 0.1525, Alpha: 1.0000
Epoch: 55, Training Loss: 0.2378, Validation Loss: 0.1928, Alpha: 1.0000
Epoch: 56, Training Loss: 0.1971, Validation Loss: 0.1711, Alpha: 1.0000
Epoch: 57, Training Loss: 0.1379, Validation Loss: 0.1466, Alpha: 1.0000
Epoch: 58, Training Loss: 0.1363, Validation Loss: 0.1250, Alpha: 1.0000
Epoch: 59, Training Loss: 0.1149, Validation Loss: 0.1168, Alpha: 1.0000
Epoch: 60, Training Loss: 0.1094, Validation Loss: 0.1045, Alpha: 1.0000
Epoch: 61, Training Loss: 0.0872, Validation Loss: 0.1044, Alpha: 1.0000
Epoch: 62, Training Loss: 0.0807, Validation Loss: 0.1068, Alpha: 1.0000
Epoch: 63, Training Loss: 0.1024, Validation Loss: 0.1122, Alpha: 1.0000
Epoch: 64, Training Loss: 0.0786, Validation Loss: 0.1114, Alpha: 1.0000
Epoch: 65, Training Loss: 0.0887, Validation Loss: 0.1191, Alpha: 1.0000
Epoch: 66, Training Loss: 0.0665, Validation Loss: 0.1034, Alpha: 1.0000
Epoch: 67, Training Loss: 0.0722, Validation Loss: 0.1040, Alpha: 1.0000
Epoch: 68, Training Loss: 0.0794, Validation Loss: 0.1020, Alpha: 1.0000
Epoch: 69, Training Loss: 0.0665, Validation Loss: 0.1211, Alpha: 1.0000
Epoch: 70, Training Loss: 0.0716, Validation Loss: 0.1169, Alpha: 1.0000
Epoch: 71, Training Loss: 0.0576, Validation Loss: 0.1080, Alpha: 1.0000
Epoch: 72, Training Loss: 0.0589, Validation Loss: 0.1025, Alpha: 1.0000
Epoch: 73, Training Loss: 0.0594, Validation Loss: 0.1121, Alpha: 1.0000
Epoch: 74, Training Loss: 0.0700, Validation Loss: 0.1028, Alpha: 1.0000
Epoch: 75, Training Loss: 0.0780, Validation Loss: 0.1040, Alpha: 1.0000
Epoch: 76, Training Loss: 0.0544, Validation Loss: 0.1066, Alpha: 1.0000
Epoch: 77, Training Loss: 0.0544, Validation Loss: 0.1124, Alpha: 1.0000
Epoch: 78, Training Loss: 0.0627, Validation Loss: 0.1144, Alpha: 1.0000
Epoch: 79, Training Loss: 0.0420, Validation Loss: 0.1061, Alpha: 1.0000
Epoch: 80, Training Loss: 0.0417, Validation Loss: 0.1087, Alpha: 1.0000
Epoch: 81, Training Loss: 0.0695, Validation Loss: 0.1109, Alpha: 1.0000
Epoch: 82, Training Loss: 0.0645, Validation Loss: 0.1094, Alpha: 1.0000
Epoch: 83, Training Loss: 0.0595, Validation Loss: 0.1110, Alpha: 1.0000
Epoch: 84, Training Loss: 0.0569, Validation Loss: 0.1052, Alpha: 1.0000
Epoch: 85, Training Loss: 0.0490, Validation Loss: 0.1049, Alpha: 1.0000
Epoch: 86, Training Loss: 0.0612, Validation Loss: 0.1153, Alpha: 1.0000
Epoch: 87, Training Loss: 0.0384, Validation Loss: 0.1087, Alpha: 1.0000
Epoch: 88, Training Loss: 0.0646, Validation Loss: 0.1109, Alpha: 1.0000
Early stopping triggered.
Experiment finished at Tue Jun 25 11:20:06 CEST 2024
