{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Ablations on Fjord\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# extract test data\n",
    "df = pd.read_parquet('data/segmented-objects-dataset.parquet')\n",
    "\n",
    "# Assuming df is your DataFrame with all data\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Local version of get labels\n",
    "def get_hierarchical_labels(species_index, species_names, genus_names, class_names, binary_names, root):\n",
    "    if species_index == -1:\n",
    "        return -1, -1, -1  # Handle cases where species_index is invalid\n",
    "\n",
    "    species_name = species_names[species_index]\n",
    "    node = next((n for n in root.descendants if n.name == species_name), None)\n",
    "\n",
    "    if node is None:\n",
    "        return -1, -1, -1  # Species not found in the tree\n",
    "\n",
    "    genus_index, class_index, binary_index = -1, -1, -1\n",
    "    current_node = node\n",
    "    while current_node.parent is not None:\n",
    "        current_node = current_node.parent\n",
    "        if current_node.rank == 'genus':\n",
    "            genus_index = genus_names.index(current_node.name) if current_node.name in genus_names else -1\n",
    "        elif current_node.rank == 'class':\n",
    "            class_index = class_names.index(current_node.name) if current_node.name in class_names else -1\n",
    "        elif current_node.rank == 'binary':\n",
    "            binary_index = binary_names.index(current_node.name) if current_node.name in binary_names else -1\n",
    "\n",
    "    return genus_index, class_index, binary_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training log file\n",
    "training_log = pd.read_csv('models/ablations/attention_removed/logs/model_alpha_0.00.csv')\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(training_log['Epoch'], training_log['Training Loss'], label='Training Loss')\n",
    "plt.plot(training_log['Epoch'], training_log['Validation Loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict\n",
    "from anytree.importer import JsonImporter\n",
    "from utils.custom_dataset import CustomDataset\n",
    "\n",
    "def run_experiment(data_path, classes_file, model_path, ablation, root):\n",
    "    df = pd.read_parquet(data_path)\n",
    "    _, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "    object_names = [line.strip() for line in open(classes_file, 'r')]\n",
    "    subcategory_names, category_names, binary_names = [], [], []\n",
    "    for node in root.descendants:\n",
    "        if node.rank == 'genus':\n",
    "            subcategory_names.append(node.name)\n",
    "        elif node.rank == 'class':\n",
    "            category_names.append(node.name)\n",
    "        elif node.rank == 'binary':\n",
    "            binary_names.append(node.name)\n",
    "\n",
    "    rank_counts = defaultdict(int)\n",
    "    for node in root.descendants:\n",
    "        rank_counts[node.rank] += 1\n",
    "    num_classes_hierarchy = [rank_counts['binary'], rank_counts['class'], rank_counts['genus'], rank_counts['species']]\n",
    "\n",
    "  # Adjust model import and initialization based on ablation type\n",
    "    if ablation == 'remove_features':\n",
    "        from models.ablations.remove_features.hierarchical_cnn import HierarchicalCNN\n",
    "        model = HierarchicalCNN(num_classes_hierarchy)\n",
    "    elif ablation == 'attention_removed':\n",
    "        from models.ablations.attention_removed.hierarchical_cnn import HierarchicalCNN\n",
    "        num_additional_features = 3  # e.g., conf, iou, pred_species\n",
    "        model = HierarchicalCNN(num_classes_hierarchy, num_additional_features)\n",
    "    elif ablation == 'decreased_branch_complexity':\n",
    "        from models.ablations.decreased_branch_complexity.hierarchical_cnn import HierarchicalCNN\n",
    "        num_additional_features = 3  # e.g., conf, iou, pred_species\n",
    "        model = HierarchicalCNN(num_classes_hierarchy, num_additional_features)\n",
    "    elif ablation == 'increased_branch_complexity':\n",
    "        from models.ablations.increased_features_complexity.hierarchical_cnn import HierarchicalCNN\n",
    "        num_additional_features = 3  # e.g., conf, iou, pred_species\n",
    "        model = HierarchicalCNN(num_classes_hierarchy, num_additional_features)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported ablation study: {ablation}\")\n",
    "\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_dataset = CustomDataset(test_df, object_names, subcategory_names, category_names, binary_names, root)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    true_labels = {level: [] for level in ['binary', 'class', 'genus', 'species']}\n",
    "    predictions = {level: [] for level in ['binary', 'class', 'genus', 'species']}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, conf, iou, pred_species, species_index, genus_index, class_index, binary_index in test_loader:\n",
    "            images, conf, iou, pred_species = [x.to(device) for x in [images, conf, iou, pred_species]]\n",
    "\n",
    "            outputs = model(images, conf, iou, pred_species) if ablation != 'remove_features' else model(images)\n",
    "\n",
    "            for i, output in enumerate(outputs):\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                level = ['binary', 'class', 'genus', 'species'][i]\n",
    "                predictions[level].extend(predicted.cpu().numpy())\n",
    "\n",
    "            # Store true labels directly from the DataLoader\n",
    "            true_labels['binary'].extend(binary_index.cpu().numpy())\n",
    "            true_labels['class'].extend(class_index.cpu().numpy())\n",
    "            true_labels['genus'].extend(genus_index.cpu().numpy())\n",
    "            true_labels['species'].extend(species_index.cpu().numpy())\n",
    "\n",
    "    f1_scores = {level: f1_score(true_labels[level], predictions[level], average='macro') for level in ['binary', 'class', 'genus', 'species']}\n",
    "    return f1_scores\n",
    "\n",
    "# Populate Taxonomy\n",
    "importer = JsonImporter()\n",
    "with open('/mnt/RAID/projects/FjordVision/data/ontology.json', 'r') as f:\n",
    "    root = importer.read(f)\n",
    "\n",
    "# Paths and setup\n",
    "data_path = 'data/segmented-objects-dataset.parquet'\n",
    "classes_file = '/mnt/RAID/datasets/The Fjord Dataset/fjord/classes.txt'\n",
    "ablations = ['remove_features', 'attention_removed', 'decreased_branch_complexity', 'increased_branch_complexity']\n",
    "alpha_values = [0, 0.2, 0.5, 0.8, 1]\n",
    "\n",
    "# Run experiments and collect results\n",
    "results = []\n",
    "for ablation in ablations:\n",
    "    for alpha in alpha_values:\n",
    "        model_path = f'models/ablations/{ablation}/weights/best_model_alpha_{alpha:.2f}.pth'\n",
    "        f1_scores = run_experiment(data_path, classes_file, model_path, ablation, root)\n",
    "        results.append({'Ablation': ablation, 'Alpha': alpha, **f1_scores})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Ablations on COCO\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from anytree.importer import JsonImporter\n",
    "from collections import defaultdict\n",
    "from utils.custom_dataset import CustomDatasetCoco\n",
    "\n",
    "def run_coco_experiment(data_path, classes_file, model_path, ablation, root_coco):\n",
    "    df = pd.read_parquet(data_path)\n",
    "    _, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "    object_names = [line.strip() for line in open(classes_file, 'r')]\n",
    "    subcategory_names, category_names, binary_names = [], [], []\n",
    "    for node in root_coco.descendants:\n",
    "        if node.rank == 'subcategory':\n",
    "            subcategory_names.append(node.name)\n",
    "        elif node.rank == 'category':\n",
    "            category_names.append(node.name)\n",
    "        elif node.rank == 'binary':\n",
    "            binary_names.append(node.name)\n",
    "\n",
    "    rank_counts = defaultdict(int)\n",
    "    for node in root_coco.descendants:\n",
    "        rank_counts[node.rank] += 1\n",
    "    num_classes_hierarchy = [rank_counts['binary'], rank_counts['category'], rank_counts['subcategory'], rank_counts['object']]\n",
    "\n",
    "  # Adjust model import and initialization based on ablation type\n",
    "    if ablation == 'remove_features':\n",
    "        from models.ablations.remove_features.hierarchical_cnn import HierarchicalCNN\n",
    "        model = HierarchicalCNN(num_classes_hierarchy)\n",
    "    elif ablation == 'attention_removed':\n",
    "        from models.ablations.attention_removed.hierarchical_cnn import HierarchicalCNN\n",
    "        num_additional_features = 3  # e.g., conf, iou, pred_species\n",
    "        model = HierarchicalCNN(num_classes_hierarchy, num_additional_features)\n",
    "    elif ablation == 'decreased_branch_complexity':\n",
    "        from models.ablations.decreased_branch_complexity.hierarchical_cnn import HierarchicalCNN\n",
    "        num_additional_features = 3  # e.g., conf, iou, pred_species\n",
    "        model = HierarchicalCNN(num_classes_hierarchy, num_additional_features)\n",
    "    elif ablation == 'increased_branch_complexity':\n",
    "        from models.ablations.increased_features_complexity.hierarchical_cnn import HierarchicalCNN\n",
    "        num_additional_features = 3  # e.g., conf, iou, pred_species\n",
    "        model = HierarchicalCNN(num_classes_hierarchy, num_additional_features)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported ablation study: {ablation}\")\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_dataset = CustomDatasetCoco(test_df, object_names, subcategory_names, category_names, binary_names, root_coco)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    true_labels = {level: [] for level in ['binary', 'category', 'subcategory', 'object']}\n",
    "    predictions = {level: [] for level in ['binary', 'category', 'subcategory', 'object']}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, conf, iou, pred_species, species_index, genus_index, class_index, binary_index in test_loader:\n",
    "            images, conf, iou, pred_species = [x.to(device) for x in [images, conf, iou, pred_species]]\n",
    "\n",
    "            outputs = model(images, conf, iou, pred_species) if ablation != 'remove_features' else model(images)\n",
    "\n",
    "            for i, output in enumerate(outputs):\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                level = ['binary', 'category', 'subcategory', 'object'][i]\n",
    "                predictions[level].extend(predicted.cpu().numpy())\n",
    "\n",
    "            # Store true labels directly from the DataLoader\n",
    "            true_labels['binary'].extend(binary_index.cpu().numpy())\n",
    "            true_labels['category'].extend(class_index.cpu().numpy())\n",
    "            true_labels['subcategory'].extend(genus_index.cpu().numpy())\n",
    "            true_labels['object'].extend(species_index.cpu().numpy())\n",
    "\n",
    "    f1_scores = {level: f1_score(true_labels[level], predictions[level], average='macro') for level in ['binary', 'category', 'subcategory', 'object']}\n",
    "    return f1_scores\n",
    "\n",
    "# Populate Taxonomy for COCO\n",
    "importer = JsonImporter()\n",
    "root_coco = importer.read(open('data/coco.json', 'r'))\n",
    "\n",
    "# Paths and setup for COCO\n",
    "data_path = 'data/coco-segmented-objects-dataset.parquet'\n",
    "classes_file = '/mnt/RAID/datasets/coco/classes.txt'\n",
    "ablations = ['remove_features', 'attention_removed', 'decreased_branch_complexity', 'increased_branch_complexity']\n",
    "alpha_values = [0, 0.2, 0.5, 0.8, 1]\n",
    "\n",
    "# Run experiments and collect results for COCO\n",
    "coco_results = []\n",
    "for ablation in ablations:\n",
    "    for alpha in alpha_values:\n",
    "        model_path = f'models/ablations/{ablation}/weights-coco/best_model_alpha_{alpha:.2f}.pth'\n",
    "        f1_scores = run_coco_experiment(data_path, classes_file, model_path, ablation, root_coco)\n",
    "        coco_results.append({'Ablation': ablation, 'Alpha': alpha, **f1_scores})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "coco_results_df = pd.DataFrame(coco_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fjordvision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
